{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nitrc.org/plugins/mwiki/index.php/neurobureau:AthenaPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from Code.data_generator import FMRIDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Conv3D, MaxPool3D, TimeDistributed, Flatten, LSTM, Dense\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract file names for your data in an excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 'D:\\adhd-fmri\\model_data\\nii_gz_file_list.xlsx' with 222 file names.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import openpyxl # For writing .xlsx files\n",
    "\n",
    "def create_excel_of_nii_gz_files(folder_path_str: str, output_excel_name: str = \"nii_gz_file_list.xlsx\"):\n",
    "    \"\"\"\n",
    "    Scans a folder for .nii.gz files and creates an Excel sheet listing them.\n",
    "\n",
    "    Args:\n",
    "        folder_path_str (str): The path to the folder to scan.\n",
    "        output_excel_name (str): The name for the output Excel file.\n",
    "                                 This will be created inside the specified folder_path_str.\n",
    "    \"\"\"\n",
    "    folder_path = Path(folder_path_str)\n",
    "\n",
    "    # 1. Validate the folder path\n",
    "    if not folder_path.is_dir():\n",
    "        print(f\"Error: Folder not found at '{folder_path_str}'\")\n",
    "        return\n",
    "\n",
    "    # 2. Find all .nii.gz files\n",
    "    #    glob(\"*.nii.gz\") finds files matching the pattern directly in the folder.\n",
    "    #    We use a list comprehension to get just the names.\n",
    "    #    sorted() ensures the list is in a predictable order.\n",
    "    nii_gz_files = sorted([file.name for file in folder_path.glob(\"*.nii.gz\") if file.is_file()])\n",
    "\n",
    "    if not nii_gz_files:\n",
    "        print(f\"No '.nii.gz' files found in '{folder_path_str}'.\")\n",
    "        return\n",
    "\n",
    "    # 3. Create an Excel workbook and select the active sheet\n",
    "    workbook = openpyxl.Workbook()\n",
    "    sheet = workbook.active\n",
    "    sheet.title = \"NII.GZ Files\" # You can customize the sheet name\n",
    "\n",
    "    # 4. Add a header to the first column\n",
    "    sheet['A1'] = \"Filename\"\n",
    "\n",
    "    # 5. Write the filenames to the sheet, starting from the second row\n",
    "    for i, filename in enumerate(nii_gz_files, start=2): # start=2 means row 2 (A2, A3, ...)\n",
    "        sheet[f'A{i}'] = filename\n",
    "        # If you wanted full paths, you could use:\n",
    "        # full_path_file = folder_path / filename\n",
    "        # sheet[f'A{i}'] = str(full_path_file)\n",
    "\n",
    "\n",
    "    # 6. Define the output path for the Excel file\n",
    "    output_excel_path = folder_path / output_excel_name\n",
    "\n",
    "    # 7. Save the workbook\n",
    "    try:\n",
    "        workbook.save(output_excel_path)\n",
    "        print(f\"Successfully created '{output_excel_path}' with {len(nii_gz_files)} file names.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Excel file: {e}\")\n",
    "        print(f\"Please ensure you have write permissions to '{folder_path}' or that the file is not open.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Method 1: Hardcode the folder path (simple for quick use) ---\n",
    "    target_folder = r\"D:/adhd-fmri/model_data\"   \n",
    "    create_excel_of_nii_gz_files(target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def parse_dx(dx):\n",
    "    if int(dx) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "# base_dir = \"../data\"\n",
    "# dataset_dir = \"../data/model_data\"\n",
    "\n",
    "base_dir = r\"D:/adhd-fmri\"\n",
    "dataset_dir = r\"D:/adhd-fmri/model_data\"  \n",
    "\n",
    "files_list = []\n",
    "for file in os.listdir(dataset_dir):\n",
    "    nums = re.findall(r'\\d+', file)\n",
    "    file_id = None\n",
    "    for num in nums: \n",
    "        if len(num) > 1: \n",
    "            file_id = int(num)\n",
    "            \n",
    "    files_list.append({\"ScanDir ID\": file_id, \"Image\": file} )\n",
    "\n",
    "images_df = pd.DataFrame(files_list)\n",
    "tsv_path = r\"D:\\Joe Workspace\\Codespace\\Github\\Diagnosing-ADHD-With-ConvLSTM\\References\\adhd200_preprocessed_phenotypics.tsv\"\n",
    "references_path = r\"D:\\Joe Workspace\\Codespace\\Github\\Diagnosing-ADHD-With-ConvLSTM\\References\"\n",
    "\n",
    "adhd_info = pd.read_csv(tsv_path, delimiter=\"\\t\")[['ScanDir ID','DX']]\n",
    "\n",
    "model_data = adhd_info.merge(images_df, on='ScanDir ID')\n",
    "\n",
    "for index,row in model_data.iterrows():\n",
    "    if row['DX'] == 'pending':\n",
    "        model_data.drop(index,axis=0,inplace=True)\n",
    "\n",
    "model_data['DX'] = model_data['DX'].apply(parse_dx)\n",
    "\n",
    "model_data.to_csv(os.path.join(references_path, \"model_data.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from scipy.ndimage import zoom\n",
    "import os\n",
    "import nibabel as nib\n",
    "\n",
    "class FMRIDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, dataset_dir, batch_size):\n",
    "        'Initialization'\n",
    "        self.x_dim = 49\n",
    "        self.y_dim = 58\n",
    "        self.z_dim = 47\n",
    "        self.time_length = 177\n",
    "        self.img_dim = (self.x_dim, self.y_dim, self.z_dim) \n",
    "        self.dim = [self.time_length, 28, 28, 28, 1] # [time, x, y, z, c]\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = 1\n",
    "        self.n_classes = 1\n",
    "        self.shuffle = True\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "                \n",
    "        for i, img_path in enumerate(list_IDs_temp):\n",
    "            X[i,] = self.preprocess_image(img_path)\n",
    "            y[i] = self.labels[img_path]\n",
    "\n",
    "        return X,y\n",
    "\n",
    "    # Image Preprocessing Methods\n",
    "    def preprocess_image(self, img_path):\n",
    "        img = nib.load(os.path.join(self.dataset_dir, img_path))\n",
    "\n",
    "        pp_img = None\n",
    "        if img.shape[3] > self.time_length:\n",
    "            pp_img = self.truncate_image(img)\n",
    "        elif img.shape[3] < self.time_length:\n",
    "            pp_img = self.pad_image(img)\n",
    "        else:\n",
    "            pp_img = img.get_fdata()\n",
    "\n",
    "        # For each image at the index-th time step, do this\n",
    "        new_x=28/49\n",
    "        new_y=28/58\n",
    "        new_z=28/47\n",
    "        \n",
    "        new_img = []\n",
    "        for index in range(self.time_length):\n",
    "            z_img = zoom(pp_img[:,:,:,index], (new_x,new_y,new_z), order=1)\n",
    "            new_img.append(z_img.reshape((28,28,28,1)))\n",
    "        \n",
    "        f_img = np.array(new_img)\n",
    "        return f_img\n",
    "    \n",
    "    def truncate_image(self, img):\n",
    "        return img.get_fdata()[:,:,:,:self.time_length]\n",
    "\n",
    "    def pad_image(self, img):\n",
    "        img_padding = np.expand_dims(np.zeros((self.x_dim,self.y_dim,self.z_dim)), axis=3)\n",
    "        amt_to_fill = self.time_length - img.get_fdata().shape[3]\n",
    "        padded_img = img.get_fdata()\n",
    "        for _ in range(amt_to_fill):\n",
    "            padded_img = np.append(arr=padded_img, values=img_padding, axis=3)\n",
    "\n",
    "        return padded_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets Do It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ DATA WORK ============================\n",
    "\n",
    "file_num = sys.argv[1]\n",
    "\n",
    "# Dataframes\n",
    "dataset_dir =  r\"D:/adhd-fmri/model_data\"  \n",
    "model_train_data = pd.read_csv(r\"D:/adhd-fmri/training_data.csv\".format(file_num) )\n",
    "model_val_data = pd.read_csv(r\"D:/adhd-fmri/validation_data.csv\".format(file_num) )\n",
    "\n",
    "# Dictionary of data values\n",
    "partition = {'train': model_train_data['Image'].values, \n",
    "             'validation': model_val_data['Image'].values}\n",
    "\n",
    "# Training Data\n",
    "train_labels = {}\n",
    "for index, row in model_train_data.iterrows():\n",
    "    train_labels[row['Image']] = row['DX']\n",
    "    \n",
    "# Validation Data\n",
    "val_labels = {}\n",
    "for index, row in model_val_data.iterrows():\n",
    "    val_labels[row['Image']] = row['DX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ MODEL META ============================\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 6\n",
    "input_shape=(177,28,28,28,1)\n",
    "\n",
    "train_steps_per_epoch = model_train_data.shape[0] // batch_size\n",
    "validate_steps_per_epoch = model_val_data.shape[0] // batch_size\n",
    "\n",
    "# Generators\n",
    "training_generator = FMRIDataGenerator(partition['train'], train_labels, dataset_dir, batch_size)\n",
    "validation_generator = FMRIDataGenerator(partition['validation'], val_labels, dataset_dir, batch_size)\n",
    "\n",
    "curr_time = f'{datetime.now():%H-%M-%S%z_%m%d%Y}'\n",
    "logger_path = \"/pylon5/cc5614p/deopha32/Saved_Models/adhd-fmri-history_cv{num}_{time}.csv\".format(num=file_num,time=curr_time)\n",
    "\n",
    "csv_logger = CSVLogger(logger_path, append=True)\n",
    "\n",
    "callbacks = [csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ MODEL ARCHITECTURE ============================\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    cnn_lstm_model = Sequential()\n",
    "\n",
    "    cnn_lstm_model.add(TimeDistributed(Conv3D(filters=64,kernel_size=(3,3,3),activation='relu'),\n",
    "                                  input_shape=input_shape, name=\"Input_Conv_Layer\"))\n",
    "\n",
    "    cnn_lstm_model.add(TimeDistributed(MaxPool3D(\n",
    "                                    pool_size=(2, 2, 2),\n",
    "                                    strides=(2, 2, 2),\n",
    "                                    padding='valid'\n",
    "                                    ), name=\"Pool_Layer_1\"))\n",
    "\n",
    "    cnn_lstm_model.add(TimeDistributed(Flatten(), name=\"Flatten_Layer\"))\n",
    "    \n",
    "with tf.device('/cpu:0'):\n",
    "\n",
    "    cnn_lstm_model.add(LSTM(10, dropout = 0.3, recurrent_dropout = 0.3, name=\"LSTM_Layer\"))\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    cnn_lstm_model.add(Dense(1, activation = 'sigmoid', name=\"Output_Dense_Layer\"))\n",
    "\n",
    "    cnn_lstm_model.compile(optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# cnn_lstm_model.fit_generator(generator=training_generator,\n",
    "#     steps_per_epoch=train_steps_per_epoch, verbose=1, callbacks=callbacks,\n",
    "#     validation_data=validation_generator, validation_steps=validate_steps_per_epoch,\n",
    "#     epochs=epochs)\n",
    "\n",
    "# cnn_lstm_model.fit(\n",
    "#     x=training_generator,\n",
    "#     steps_per_epoch=train_steps_per_epoch,\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=validate_steps_per_epoch,\n",
    "#     epochs=epochs,\n",
    "#     verbose=1,\n",
    "#     callbacks=callbacks,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/46216013/9221241\n",
    "def get_model_memory_usage(batch_size, model):\n",
    "    import numpy as np\n",
    "    from keras import backend as K\n",
    "\n",
    "    shapes_mem_count = 0\n",
    "    internal_model_mem_count = 0\n",
    "    for l in model.layers:\n",
    "        layer_type = l.__class__.__name__\n",
    "        if layer_type == 'Model':\n",
    "            internal_model_mem_count += get_model_memory_usage(batch_size, l)\n",
    "        # Ensure output shape exists\n",
    "        if hasattr(l, 'output_shape'):\n",
    "            out_shape = l.output_shape\n",
    "        elif hasattr(l, 'compute_output_shape'):\n",
    "            out_shape = l.compute_output_shape(l.input_shape)\n",
    "        else:\n",
    "            continue  # skip layers without shape information\n",
    "\n",
    "        single_layer_mem = 1\n",
    "        # for s in l.output_shape:\n",
    "        #     if s is None:\n",
    "        #         continue\n",
    "        #     single_layer_mem *= s\n",
    "        # shapes_mem_count += single_layer_mem\n",
    "\n",
    "    trainable_count = 2\n",
    "    non_trainable_count = 1\n",
    "    # trainable_count = np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n",
    "    # non_trainable_count = np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n",
    "\n",
    "    number_size = 4.0\n",
    "    if K.floatx() == 'float16':\n",
    "         number_size = 2.0\n",
    "    if K.floatx() == 'float64':\n",
    "         number_size = 8.0\n",
    "\n",
    "    total_memory = number_size*(batch_size*shapes_mem_count + trainable_count + non_trainable_count)\n",
    "    gbytes = np.round(total_memory / (1024.0 ** 3), 3) + internal_model_mem_count\n",
    "    return gbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_memory_usage(32, cnn_lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input_Conv_Layer                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">177</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Pool_Layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">177</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>,    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Flatten_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">177</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140608</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ LSTM_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,624,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Dense_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input_Conv_Layer                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m177\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m,    │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │ \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Pool_Layer_1 (\u001b[38;5;33mTimeDistributed\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m177\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m,    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Flatten_Layer (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m177\u001b[0m, \u001b[38;5;34m140608\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ LSTM_Layer (\u001b[38;5;33mLSTM\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │     \u001b[38;5;34m5,624,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Dense_Layer (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m11\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,626,563</span> (21.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,626,563\u001b[0m (21.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,626,563</span> (21.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,626,563\u001b[0m (21.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_lstm_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
